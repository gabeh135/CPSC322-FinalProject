{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import plot_utils\n",
    "importlib.reload(plot_utils)\n",
    "import plot_utils as plot_utils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MyDummyClassifier, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ski_dataset = MyPyTable().load_from_file(\"./input_data/ski-resorts.csv\")\n",
    "\n",
    "# exploring different attribute combinations\n",
    "ski_X = ski_dataset.get_columns([\"elevation_top_m\", \"number_of_slopes\", \"annual_snowfall_cm\"])\n",
    "ski_y = ski_dataset.get_column(\"rating\")\n",
    "\n",
    "matrix_headers = [\"rating\", \"high\", \"above average\", \"average\", \"below average\", \"low\", \"Total\", \"Recognition (%)\"]\n",
    "\n",
    "discretizer = myutils.discretize_ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN classification of ski dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['average', 'average', 'low', 'below average', 'above average', 'below average', 'low', 'low', 'below average', 'below average', 'low', 'above average', 'average', 'high', 'below average', 'average', 'above average', 'below average', 'average', 'low']\n",
      "['average', 'below average', 'low', 'low', 'average', 'below average', 'low', 'low', 'below average', 'low', 'average', 'above average', 'below average', 'above average', 'below average', 'average', 'average', 'average', 'average', 'low']\n",
      "Accuracy: 0.3848964677222899, Error Rate: 0.6151035322777101\n",
      "\n",
      "rating           high    above average    average    below average    low    Total    Recognition (%)\n",
      "-------------  ------  ---------------  ---------  ---------------  -----  -------  -----------------\n",
      "high              146              202        114               26      5      493                 29\n",
      "above average      28              137        317              138     38      658                 20\n",
      "average             5               60        383              313    225      986                 38\n",
      "below average       2                8         98              248    300      656                 37\n",
      "low                 0                1         21              119    350      491                 71\n"
     ]
    }
   ],
   "source": [
    "# TODO: modify categorical method in kNN to be a given list\n",
    "knn_clf = MyKNeighborsClassifier()\n",
    "\n",
    "knn_actual, knn_pred, knn_accuracy, knn_error = myevaluation.cross_val_predict\\\n",
    "    (ski_X, ski_y, knn_clf, discretizer=discretizer, n_splits=10, shuffle=True, stratify=True)\n",
    "\n",
    "myutils.randomize_in_place(knn_actual, knn_pred)\n",
    "print(knn_actual[:20])\n",
    "print(knn_pred[:20])\n",
    "\n",
    "print(f\"Accuracy: {knn_accuracy}, Error Rate: {knn_error}\")\n",
    "print()\n",
    "\n",
    "matrix = myevaluation.confusion_matrix(knn_actual, knn_pred, matrix_headers[1:6])\n",
    "print(myevaluation.tabulate_confusion_matrix(matrix, headers=matrix_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['high', 'high', 'high']\n",
      "[3899.0, 69.0, 450.0]\n",
      "['below average', 'average', 'average', 'average', 'low', 'above average', 'above average', 'below average', 'low', 'above average', 'low', 'above average', 'low', 'average', 'average', 'average', 'low', 'low', 'average', 'average']\n",
      "['low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low', 'low']\n",
      "Accuracy: 0.15225334957369063, Error Rate: 0.8477466504263094\n",
      "\n",
      "rating           high    above average    average    below average    low    Total    Recognition (%)\n",
      "-------------  ------  ---------------  ---------  ---------------  -----  -------  -----------------\n",
      "high                0                0          1               34    458      493                  0\n",
      "above average       0                2         14               38    604      658                  0\n",
      "average             0               16         37               31    902      986                  3\n",
      "below average       0               11         39                6    600      656                  0\n",
      "low                 0                3         31                2    455      491                 92\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "nb_X = copy.deepcopy(ski_X)\n",
    "for row in nb_X:\n",
    "    row[0] = myutils.discretize_elevation(row[0])\n",
    "    row[1] = myutils.discretize_num_slopes(row[1])\n",
    "    row[2] = myutils.discretize_snowfall(row[2])\n",
    "print(nb_X[1])\n",
    "print(ski_X[1])\n",
    "nb_clf = MyNaiveBayesClassifier()\n",
    "\n",
    "nb_actual, nb_pred, nb_accuracy, nb_error = myevaluation.cross_val_predict\\\n",
    "    (nb_X, ski_y, nb_clf, discretizer=discretizer, n_splits=10, shuffle=True, stratify=True)\n",
    "\n",
    "myutils.randomize_in_place(nb_actual, nb_pred)\n",
    "print(nb_actual[:20])\n",
    "print(nb_pred[:20])\n",
    "\n",
    "print(f\"Accuracy: {nb_accuracy}, Error Rate: {nb_error}\")\n",
    "print()\n",
    "\n",
    "matrix = myevaluation.confusion_matrix(nb_actual, nb_pred, matrix_headers[1:6])\n",
    "print(myevaluation.tabulate_confusion_matrix(matrix, headers=matrix_headers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4420091324200913, Error Rate: 0.5579908675799087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: ask Professor Sprint where part 4 of random forest should be implemented - in predict?\n",
    "# TODO: ask Professor Sprint how we should handle our categorical decision tree issue - discretizer for X attributes?\n",
    "forest_clf = MyRandomForestClassifier()\n",
    "\n",
    "# discretize the output\n",
    "y_disc = [discretizer(y) for y in ski_y]\n",
    "\n",
    "# using N=5, M=3, F=2 as a \"control\"\n",
    "X_test, y_test = forest_clf.fit(ski_X, y_disc, 5, 3, 2)\n",
    "forest_pred = forest_clf.predict(X_test)\n",
    "\n",
    "forest_accuracy = myevaluation.accuracy_score(y_test, forest_pred)\n",
    "forest_error = 1 - forest_accuracy\n",
    "\n",
    "forest_actual = y_test\n",
    "\n",
    "print(f\"Accuracy: {forest_accuracy}, Error Rate: {forest_error}\")\n",
    "print()\n",
    "\n",
    "# matrix = myevaluation.confusion_matrix(forest_actual, forest_pred, matrix_headers[1:8])\n",
    "# print(myevaluation.tabulate_confusion_matrix(matrix, headers=matrix_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues faced\n",
    "In regards to random forest, we are facing two primary issues. First, predictions sometimes are not being made. This is a bug that will likely be ironed out as we solve the next issue.\n",
    "\n",
    "\n",
    "Another issue is that continuous attributes are being treated as categorical, which leads to a decision trees with hundreds if not thousands of branches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual ['high', 'high', 'high', 'high', 'high', 'high', 'high', 'high', 'high', 'high']\n",
      "predicted ['', '', 'average', 'high', 'average', 'high', 'high', 'high', '', '']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This version tends very much towards average, but we have already addressed this by changing the distributions of the discretizer.\n",
    "print(\"actual\",y_test[:10])\n",
    "print(\"predicted\",forest_pred[:10])\n",
    "print()\n",
    "\n",
    "tree = forest_clf.forest[0]\n",
    "# tree.print_decision_rules()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncated output of tree.print_decision_rules()\n",
    "\n",
    "IF att1 == 0.0 THEN class = high  \n",
    "IF att1 == 1.0 THEN class = below average  \n",
    "IF att1 == 2.0 THEN class = average  \n",
    "IF att1 == 3.0 THEN class = average  \n",
    "IF att1 == 4.0 THEN class = average  \n",
    "IF att1 == 5.0 THEN class = average  \n",
    "IF att1 == 6.0 THEN class = average  \n",
    "IF att1 == 7.0 THEN class = above average  \n",
    "IF att1 == 8.0 THEN class = average  \n",
    "IF att1 == 9.0 THEN class = above average  \n",
    "IF att1 == 10.0 THEN class = average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m values, counts \u001b[38;5;241m=\u001b[39m myutils\u001b[38;5;241m.\u001b[39mget_frequencies(y_disc)\n\u001b[1;32m      7\u001b[0m order \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m]\n\u001b[0;32m----> 8\u001b[0m ordered_values \u001b[38;5;241m=\u001b[39m [values[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m order]\n\u001b[1;32m      9\u001b[0m ordered_counts \u001b[38;5;241m=\u001b[39m [counts[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m order]\n\u001b[1;32m     11\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# will make own method\n",
    "y_disc = [discretizer(y) for y in ski_y]\n",
    "values, counts = myutils.get_frequencies(y_disc)\n",
    "\n",
    "order = [5, 3, 0, 1, 2, 4, 6]\n",
    "ordered_values = [values[i] for i in order]\n",
    "ordered_counts = [counts[i] for i in order]\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(ordered_values, ordered_counts)\n",
    "plt.xlabel(\"ELO Rating\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(f\"Distribution of Table Classifications\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
